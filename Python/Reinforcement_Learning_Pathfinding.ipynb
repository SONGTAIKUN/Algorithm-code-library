{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbf8e6b0-4c6a-4f80-a9b3-444183677a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -302.0\n",
      "Episode 50, Total Reward: 53.0\n",
      "Episode 100, Total Reward: 57.0\n",
      "Episode 150, Total Reward: 80.0\n",
      "Episode 200, Total Reward: 81.0\n",
      "Episode 250, Total Reward: 84.0\n",
      "Episode 300, Total Reward: 87.0\n",
      "Episode 350, Total Reward: 83.0\n",
      "Episode 400, Total Reward: 77.0\n",
      "Episode 450, Total Reward: 85.0\n",
      "\n",
      "Optimal Path Found by Q-learning:\n",
      "[(0, 0), (1, 0), (1, 1), (1, 2), (1, 3), (2, 3), (3, 3), (4, 3), (4, 4), (5, 4), (5, 5), (5, 6), (5, 7), (6, 7), (7, 7)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "'''\n",
    "这个代码实现了强化学习（Reinforcement Learning, RL）中的Q-learning 算法，在 4×4 迷宫（Gridworld） 环境中训练一个智能体（Agent）找到从 (0,0) 到 (3,3) 的最佳路径。\n",
    "Q-learning 是强化学习（Reinforcement Learning, RL）中的一种无模型（Model-Free）学习算法，它可以在未知环境中通过**试错（Trial and Error）**不断优化决策，最终学到最优策略（Optimal Policy）。\n",
    "'''\n",
    "\n",
    "'''\n",
    " 强化学习 (RL) 概述\n",
    "\n",
    "强化学习是一种智能体与环境交互的学习方法，其中：\n",
    "\n",
    "环境（Environment）：一个状态空间，定义了智能体可以在其中移动。\n",
    "智能体（Agent）：在环境中学习如何行动的对象。\n",
    "动作（Actions）：智能体在每个状态下可以选择的行为。\n",
    "奖励（Reward）：智能体执行动作后获得的反馈，指导学习。\n",
    "状态（State）：智能体在某一时刻所处的情况。\n",
    "在这个代码中：\n",
    "\n",
    "环境是一个 4x4 网格迷宫，智能体从 (0,0) 出发，到达 (3,3) 为终点。\n",
    "智能体可以选择 \"UP\"、\"DOWN\"、\"LEFT\"、\"RIGHT\" 四个动作。\n",
    "每移动一步奖励 -1，终点奖励 +100，智能体要学习最快到达终点的策略。\n",
    "'''\n",
    "\n",
    "# 定义环境大小\n",
    "GRID_SIZE = 8\n",
    "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "LEARNING_RATE = 0.1   # 学习率\n",
    "DISCOUNT_FACTOR = 0.9  # 折扣因子\n",
    "EPSILON = 0.2          # 探索率（ε-greedy）\n",
    "\n",
    "# 初始化 Q 表 (4x4x4) -> 4x4 的网格，每个格子有 4 个动作的 Q 值\n",
    "Q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "\n",
    "# 奖励表\n",
    "reward_table = -1 * np.ones((GRID_SIZE, GRID_SIZE))  # 默认奖励 -1\n",
    "reward_table[GRID_SIZE - 1, GRID_SIZE - 1] = 100  # 终点奖励 +100\n",
    "\n",
    "# 定义动作的偏移量\n",
    "action_map = {\n",
    "    'UP': (-1, 0),\n",
    "    'DOWN': (1, 0),\n",
    "    'LEFT': (0, -1),\n",
    "    'RIGHT': (0, 1)\n",
    "}\n",
    "\n",
    "# 选择动作（ε-greedy）\n",
    "def choose_action(state):\n",
    "    x, y = state\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.choice(ACTIONS)  # 随机探索\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(Q_table[x, y])]  # 选择最高 Q 值的动作\n",
    "\n",
    "# 执行动作，返回新状态和奖励\n",
    "def step(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = action_map[action]\n",
    "    new_x, new_y = max(0, min(GRID_SIZE - 1, x + dx)), max(0, min(GRID_SIZE - 1, y + dy))  # 限制边界\n",
    "    return (new_x, new_y), reward_table[new_x, new_y]\n",
    "\n",
    "# Q-learning 训练\n",
    "def train(episodes=500):\n",
    "    for episode in range(episodes):\n",
    "        state = (0, 0)  # 初始位置\n",
    "        total_reward = 0\n",
    "\n",
    "        while state != (GRID_SIZE - 1, GRID_SIZE - 1):  # 直到到达终点\n",
    "            action = choose_action(state)  # 选择动作\n",
    "            next_state, reward = step(state, action)  # 执行动作\n",
    "            x, y = state\n",
    "            action_index = ACTIONS.index(action)   #获得action对应的序号：    'UP': 0；'DOWN': 1；'LEFT': 2；'RIGHT': 3.\n",
    "\n",
    "            # Q 值更新公式\n",
    "            Q_table[x, y, action_index] = (1 - LEARNING_RATE) * Q_table[x, y, action_index] + \\\n",
    "                                          LEARNING_RATE * (reward + DISCOUNT_FACTOR * np.max(Q_table[next_state])) #np.max(Q_table[next_state])：输出next_state四个方向中Q_table最大的那个值\n",
    "\n",
    "            state = next_state  # 更新状态\n",
    "            total_reward += reward\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "# 运行训练\n",
    "train()\n",
    "\n",
    "# 测试最佳路径\n",
    "def test():\n",
    "    state = (0, 0)\n",
    "    path = [state]\n",
    "    while state != (GRID_SIZE - 1, GRID_SIZE - 1):\n",
    "        action = ACTIONS[np.argmax(Q_table[state])] #每一步都选之前学习出来的Q_table最大的那一个方向\n",
    "        state, _ = step(state, action)\n",
    "        path.append(state)\n",
    "    \n",
    "    print(\"\\nOptimal Path Found by Q-learning:\")\n",
    "    print(path)\n",
    "\n",
    "# 测试训练结果\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac128e78-5de4-4fa0-9305-3a3533ed8387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
